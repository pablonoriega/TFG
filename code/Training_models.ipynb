{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Unlearning: el arte de olvidar en la era de la Inteligencia Artificial**\n"
      ],
      "metadata": {
        "id": "SNNzf4SCvQ5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descripción del Notebook\n",
        "\n",
        "En este notebook se proporciona el código para entrenar tres modelos diferentes como parte del Trabajo de Fin de Grado (TFG) titulado \"Machine Unlearning: el arte de olvidar en la era de la Inteligencia Artificial\", realizado por Pablo Noriega Vázquez:\n",
        "\n",
        "1. **Modelo Original**:\n",
        "   - Utiliza un modelo preentrenado de ResNet50 para realizar una tarea de regresión. Se carga un conjunto de datos original y se entrena el modelo utilizando el conjunto de datos sin modificaciones.\n",
        "\n",
        "2. **Modelo de Fine-Tuning Básico**:\n",
        "   - Carga el modelo original y realiza un fine-tuning borrando un rango de edades con la finalidad de aplicar un proceso de unlearning.\n",
        "\n",
        "3. **Modelo de Fine-Tuning con Etiquetas Aleatorias**:\n",
        "   - En este modelo se simula un escenario de fine-tuning más desafiante. Se carga un conjunto de datos original y se realiza un fine-tuning con etiquetas aleatorias. Esto implica una modificación selectiva de las edades en los datos de entrenamiento para mejorar la robustez del modelo.\n"
      ],
      "metadata": {
        "id": "GGoUIKIjuahA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GAYhb45xKa0e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "from zipfile import ZipFile\n",
        "import h5py\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### En esta celda de código se realiza el siguiente proceso:\n",
        "\n",
        "1. **Montar Google Drive**:\n",
        "   - Se conecta Google Drive en el entorno de ejecución de Google Colab. Esto permite acceder y manipular los archivos almacenados en Google Drive desde el entorno de Colab.\n",
        "\n",
        "2. **Instalar gdown**:\n",
        "   - Se instala la herramienta `gdown` para descargar archivos desde Google Drive directamente desde Colab.\n",
        "\n",
        "3. **Descargar y descomprimir el archivo desde Google Drive**:\n",
        "   - Se define el enlace del archivo en Google Drive a través de su ID.\n",
        "   - Se utiliza `gdown` para descargar el archivo desde Google Drive a Colab.\n",
        "   - Luego, se descomprime el archivo descargado.\n",
        "\n",
        "4. **Eliminar el archivo .zip después de la extracción**:\n",
        "   - Una vez que el archivo se ha descomprimido con éxito, se elimina el archivo `.zip` para liberar espacio en el entorno de Colab.\n",
        "\n",
        "Este proceso permite descargar y descomprimir el archivo en el que incluimos modelos y bases de datos, almacenado en Google Drive en el entorno de ejecución de Google Colab.\n"
      ],
      "metadata": {
        "id": "xgTkcHh5tpj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar Google Drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# Descargar y descomprimir el archivo desde Google Drive\n",
        "!pip install gdown\n",
        "\n",
        "# Enlace de Google Drive\n",
        "file_id = '1HtTUZnnXz5dVAPLyDo8Vhj6IyFYfxL0-'\n",
        "gdown_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Descargar el archivo\n",
        "!gdown $gdown_url -O data.zip\n",
        "\n",
        "# Descomprimir el archivo\n",
        "with ZipFile('data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "    print('Model decompressed successfully')\n",
        "\n",
        "# Eliminar el archivo .zip después de la extracción para liberar espacio\n",
        "os.remove('data.zip')\n"
      ],
      "metadata": {
        "id": "MloY9wbvY1a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENTRENAR MODELO ORIGINAL PREDICCIÓN DE EDAD**"
      ],
      "metadata": {
        "id": "yTkZltKhMXD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Cargar los datos**: Se cargan los datos de entrenamiento, validación y prueba desde archivos `.npy` ubicados en el directorio `./NoriegaVazquezPablo_TFG_CODE/originalData`.\n",
        "\n",
        "2. **Normalizar etiquetas**: Se normalizan las etiquetas dividiéndolas por 60. Esto se aplica a las etiquetas de entrenamiento (`Y_train`), validación (`Y_valid`) y prueba (`Y_test`).\n",
        "\n",
        "3. **Verificar dimensiones**: Se imprimen las dimensiones de los datos cargados para asegurar que todo se ha cargado correctamente y las dimensiones son las esperadas.\n"
      ],
      "metadata": {
        "id": "L6-bmmoldlTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los conjuntos de datos\n",
        "X_train = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'X_train.npy'))\n",
        "Y_train = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'Y_train.npy'))\n",
        "Y_train = Y_train / 60  # Normalizar etiquetas de entrenamiento\n",
        "\n",
        "X_valid = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'X_valid.npy'))\n",
        "Y_valid = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'Y_valid.npy'))\n",
        "Y_valid = Y_valid / 60  # Normalizar etiquetas de validación\n",
        "\n",
        "X_test = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'X_test.npy'))\n",
        "Y_test = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'Y_test.npy'))\n",
        "Y_test = Y_test / 60  # Normalizar etiquetas de prueba\n",
        "\n",
        "# Imprimir dimensiones de los conjuntos de datos\n",
        "print(\"Dimensiones de los conjuntos de entrenamiento:\")\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"Y_train:\", Y_train.shape)\n",
        "\n",
        "print(\"\\nDimensiones de los conjuntos de validación:\")\n",
        "print(\"X_valid:\", X_valid.shape)\n",
        "print(\"Y_valid:\", Y_valid.shape)\n",
        "\n",
        "print(\"\\nDimensiones de los conjuntos de test:\")\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"Y_test:\", Y_test.shape)\n"
      ],
      "metadata": {
        "id": "WryObOvua0Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocesar conjunto de entrenamiento, validación y prueba**:\n",
        "   - Se convierten los datos a tipo `float32` para asegurar que están en el formato correcto para la red neuronal.\n",
        "   - Para cada imagen, se expande una dimensión adicional para que tenga la forma esperada por la función `preprocess_input` de ResNet50.\n",
        "   - Luego, se aplica `preprocess_input` de ResNet50 para normalizar las imágenes de acuerdo con los requisitos del modelo preentrenado.\n",
        "\n",
        "Esta preprocesamiento es necesario porque ResNet50, como muchas redes neuronales preentrenadas, espera que las imágenes de entrada estén normalizadas de una manera específica, lo cual ayuda a mejorar la precisión y la convergencia del modelo.\n"
      ],
      "metadata": {
        "id": "4hNWt8TPd7h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "X_train = X_train.astype('float32')\n",
        "for i in range(0,X_train.shape[0]):\n",
        "  x = X_train[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_train[i,] = tf.keras.applications.resnet50.preprocess_input(x)\n",
        "\n",
        "# validation set\n",
        "X_valid = X_valid.astype('float32')\n",
        "for i in range(0,X_valid.shape[0]):\n",
        "  x = X_valid[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_valid[i,] = tf.keras.applications.resnet50.preprocess_input(x)\n",
        "\n",
        "X_test = X_test.astype('float32')\n",
        "for i in range(0,X_test.shape[0]):\n",
        "  x = X_test[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_test[i,] = tf.keras.applications.resnet50.preprocess_input(x)"
      ],
      "metadata": {
        "id": "IlNMFkOoMov-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta celda de código, se realiza lo siguiente:\n",
        "\n",
        "1. **Descarga y descompresión de datos**:\n",
        "   - Se descarga un archivo comprimido que contiene el modelo preentrenado.\n",
        "   - Luego, se descomprime el archivo para obtener el modelo.\n",
        "\n",
        "2. **Carga del modelo preentrenado**:\n",
        "   - Se carga el modelo preentrenado desde el archivo `weights.h5`.\n",
        "\n",
        "3. **Modificación y ampliación del modelo**:\n",
        "   - Se agregan capas de dropout para evitar el sobreajuste.\n",
        "   - Se añaden capas Fully Connected (FC) ocultas para aprender representaciones ocultas.\n",
        "   - Se configura una capa de salida para un problema de regresión.\n",
        "\n",
        "4. **Construcción del modelo final**:\n",
        "   - Se construye el modelo final que incluye las modificaciones realizadas.\n"
      ],
      "metadata": {
        "id": "maEdueGifBNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar los datos\n",
        "!wget http://data.chalearnlap.cvc.uab.cat/Colab_MFPDS/model.zip\n",
        "\n",
        "# Descomprimir los datos\n",
        "with ZipFile('model.zip', 'r') as zip:\n",
        "   zip.extractall()\n",
        "   print('Modelo descomprimido exitosamente')\n",
        "\n",
        "# Eliminar el archivo .zip después de la extracción para liberar espacio\n",
        "!rm model.zip\n",
        "\n",
        "# Cargar el modelo preentrenado\n",
        "model = tf.keras.models.load_model('./model/weights.h5')\n",
        "\n",
        "# Utilizar la capa FC antes de la capa 'classifier_low_dim' como vector de características\n",
        "fc_512 = model.get_layer('dim_proj').output\n",
        "\n",
        "# Agregar una capa de dropout para minimizar problemas de sobreajuste\n",
        "dp_layer = tf.keras.layers.Dropout(0.5)(fc_512)\n",
        "\n",
        "# Agregar algunas capas FC ocultas para aprender representaciones ocultas\n",
        "fc_256 = tf.keras.layers.Dense(256, activation='relu', name='f_256')(dp_layer)\n",
        "\n",
        "# Agregar otra capa de dropout para minimizar problemas de sobreajuste\n",
        "dp_layer2 = tf.keras.layers.Dropout(0.2)(fc_256)\n",
        "fc_128 = tf.keras.layers.Dense(128, activation='relu', name='f_128')(dp_layer2)\n",
        "\n",
        "# Para regresión, típicamente usamos un solo neurona en la capa de salida con activación lineal\n",
        "output_aux = tf.keras.layers.Dense(1, name='output')(fc_128)\n",
        "output = tf.keras.layers.Activation('sigmoid', name='predict')(output_aux)\n",
        "\n",
        "# Construir e imprimir el modelo final\n",
        "model = tf.keras.models.Model(inputs=model.get_layer('base_input').output, outputs=output)\n"
      ],
      "metadata": {
        "id": "Sk6h_7vsMth6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta celda de código, se realiza lo siguiente:\n",
        "\n",
        "1. **Configuración de parámetros**:\n",
        "   - Se definen los parámetros para el entrenamiento del modelo, como el número de épocas (`NUM_EPOCHS`), la tasa de aprendizaje (`lr`) y el tamaño del lote (`batch_s`).\n",
        "\n",
        "2. **Ruta de guardado del modelo**:\n",
        "   - Se define la ruta donde se guardará el modelo entrenado (`model_name`).\n",
        "\n",
        "3. **Definición de generadores de datos**:\n",
        "   - Se utilizan generadores de datos para el conjunto de entrenamiento y validación, lo que permite aplicar el aumento de datos de forma dinámica durante el entrenamiento.\n",
        "\n",
        "4. **Compilación del modelo**:\n",
        "   - Se compila el modelo utilizando el optimizador Adam y la función de pérdida de error absoluto medio (`mean_absolute_error`).\n",
        "\n",
        "5. **Definición de callbacks**:\n",
        "   - Se definen callbacks, como `EarlyStopping` para detener el entrenamiento prematuramente si la pérdida en el conjunto de validación deja de disminuir, y `ModelCheckpoint` para guardar el mejor modelo obtenido durante el entrenamiento.\n",
        "\n",
        "6. **Entrenamiento del modelo**:\n",
        "   - Se entrena el modelo utilizando los generadores de datos de entrenamiento y validación, así como los parámetros definidos anteriormente y los callbacks especificados.\n",
        "\n",
        "7. **Guardado del historial de entrenamiento**:\n",
        "    - Se guarda el historial de entrenamiento del modelo en un archivo para su posterior análisis.\n"
      ],
      "metadata": {
        "id": "sRAUepiFhLmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Número de epochs de entrenamiento\n",
        "NUM_EPOCHS = 120\n",
        "\n",
        "# Tasa de aprendizaje\n",
        "lr = 1e-5\n",
        "\n",
        "# Tamaño del lote (batch size)\n",
        "batch_s = 32\n",
        "\n",
        "#Ruta donde se guardará el modelo\n",
        "model_name = '/content/gdrive/MyDrive/original_model.h5'\n",
        "\n",
        "# Definir parámetros de aumento de datos\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "# Convertir las etiquetas one-hot a etiquetas de clase\n",
        "# y_train_classes = np.argmax(Y_train_onehot, axis=1)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mean_absolute_error')\n",
        "\n",
        "# Definir el generador para los datos de entrenamiento con aumento de datos\n",
        "train_generator = datagen.flow(X_train, Y_train, batch_size=batch_s)\n",
        "valid_generator = datagen.flow(X_valid, Y_valid, batch_size=batch_s)\n",
        "\n",
        "# Definir los callbacks\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
        "mc = tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(train_generator, validation_data=valid_generator, batch_size=batch_s, epochs=NUM_EPOCHS, shuffle=True, verbose=1, callbacks=[es, mc])\n",
        "\n",
        "# Guardar el historial de entrenamiento\n",
        "with open(model_name, 'wb') as handle:\n",
        "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "7SvIHK3VM6c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENTRENAR MODELO UNLEARNING 1 (FINE-TUNING BÁSICO)**"
      ],
      "metadata": {
        "id": "C_hzh0FjRVhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Cargar los datos**: Se cargan los datos de entrenamiento, validación y prueba desde archivos `.npy` ubicados en el directorio `./NoriegaVazquezPablo_TFG_CODE/dataWithout_20-28`. En este conjunto se han borrado las muestras entre 20 y 28 años.\n",
        "\n",
        "2. **Normalizar etiquetas**: Se normalizan las etiquetas dividiéndolas por 60. Esto se aplica a las etiquetas de entrenamiento (`Y_train`), validación (`Y_valid`) y prueba (`Y_test`).\n",
        "\n",
        "3. **Verificar dimensiones**: Se imprimen las dimensiones de los datos cargados para asegurar que todo se ha cargado correctamente y las dimensiones son las esperadas."
      ],
      "metadata": {
        "id": "oIBpMcNsjXuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los conjuntos de datos\n",
        "\n",
        "# Cargar el conjunto de entrenamiento\n",
        "X_train = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/dataWithout_20-28', 'X_train_without_20-28.npy'))\n",
        "Y_train = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/dataWithout_20-28', 'Y_train_without_20-28.npy'))\n",
        "Y_train = Y_train / 60  # Normalizar etiquetas de entrenamiento\n",
        "\n",
        "# Cargar el conjunto de validación\n",
        "X_valid = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/dataWithout_20-28', 'X_valid_without_20-28.npy'))\n",
        "Y_valid = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/dataWithout_20-28', 'Y_valid_without_20-28.npy'))\n",
        "Y_valid = Y_valid / 60  # Normalizar etiquetas de entrenamiento\n",
        "\n",
        "# Cargar el conjunto de prueba\n",
        "X_test = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/dataWithout_20-28', 'X_test_without_20-28.npy'))\n",
        "Y_test = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/dataWithout_20-28', 'Y_test_without_20-28.npy'))\n",
        "Y_test = Y_test / 60  # Normalizar etiquetas de entrenamiento\n",
        "\n",
        "# Imprimir las dimensiones de los conjuntos de datos\n",
        "print(\"Dimensiones de los conjuntos de entrenamiento:\")\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"Y_train:\", Y_train.shape)\n",
        "\n",
        "print(\"\\nDimensiones de los conjuntos de validación:\")\n",
        "print(\"X_valid:\", X_valid.shape)\n",
        "print(\"Y_valid:\", Y_valid.shape)\n",
        "\n",
        "print(\"\\nDimensiones de los conjuntos de test:\")\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"Y_test:\", Y_test.shape)"
      ],
      "metadata": {
        "id": "9mC_mJOHRdjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocesar conjunto de entrenamiento, validación y prueba**:\n",
        "   - Se convierten los datos a tipo `float32` para asegurar que están en el formato correcto para la red neuronal.\n",
        "   - Para cada imagen, se expande una dimensión adicional para que tenga la forma esperada por la función `preprocess_input` de ResNet50.\n",
        "   - Luego, se aplica `preprocess_input` de ResNet50 para normalizar las imágenes de acuerdo con los requisitos del modelo preentrenado.\n",
        "\n",
        "Esta preprocesamiento es necesario porque ResNet50, como muchas redes neuronales preentrenadas, espera que las imágenes de entrada estén normalizadas de una manera específica, lo cual ayuda a mejorar la precisión y la convergencia del modelo.\n"
      ],
      "metadata": {
        "id": "slpdwMzsjsU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "X_train = X_train.astype('float32')\n",
        "for i in range(0,X_train.shape[0]):\n",
        "  x = X_train[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_train[i,] = tf.keras.applications.resnet50.preprocess_input(x)\n",
        "\n",
        "# validation set\n",
        "X_valid = X_valid.astype('float32')\n",
        "for i in range(0,X_valid.shape[0]):\n",
        "  x = X_valid[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_valid[i,] = tf.keras.applications.resnet50.preprocess_input(x)\n",
        "\n",
        "X_test = X_test.astype('float32')\n",
        "for i in range(0,X_test.shape[0]):\n",
        "  x = X_test[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_test[i,] = tf.keras.applications.resnet50.preprocess_input(x)"
      ],
      "metadata": {
        "id": "SkKc9N7pRtIC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###En esta celda de código, se llevan a cabo las siguientes acciones:\n",
        "\n",
        "1. **Definición de hiperparámetros**:\n",
        "   - Se define el número de epochs de entrenamiento (`NUM_EPOCHS`), la tasa de aprendizaje (`lr`), y el tamaño del lote (`batch_s`).\n",
        "\n",
        "2. **Cargar el modelo preentrenado**:\n",
        "   - Se carga el modelo preentrenado desde la ubicación especificada.\n",
        "\n",
        "3. **Definir parámetros de aumento de datos**:\n",
        "   - Se utilizan los parámetros de aumento de datos definidos mediante `ImageDataGenerator()`.\n",
        "\n",
        "4. **Compilar el modelo**:\n",
        "   - Se compila el modelo utilizando el optimizador Adam con la tasa de aprendizaje especificada y la función de pérdida de error absoluto medio (`mean_absolute_error`).\n",
        "\n",
        "5. **Definir el generador para los datos de entrenamiento y validación**:\n",
        "   - Se definen los generadores de datos para los conjuntos de entrenamiento y validación utilizando los parámetros de aumento de datos y el tamaño del lote especificados.\n",
        "\n",
        "6. **Definir los callbacks**:\n",
        "   - Se establecen los callbacks, incluyendo `EarlyStopping` para detener el entrenamiento si la pérdida en el conjunto de validación deja de disminuir, y `ModelCheckpoint` para guardar el mejor modelo obtenido durante el entrenamiento.\n",
        "\n",
        "7. **Entrenar el modelo**:\n",
        "   - Se entrena el modelo utilizando los generadores de datos de entrenamiento y validación, el número de épocas especificado y los callbacks definidos.\n",
        "\n",
        "8. **Guardar el historial de entrenamiento**:\n",
        "   - Se guarda el historial de entrenamiento en un archivo.\n",
        "\n",
        "9. **Guardar el modelo final**:\n",
        "   - Se guarda el modelo final en un archivo con el nombre especificado.\n"
      ],
      "metadata": {
        "id": "Le-4ccwJk2Sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Definir los parámetros de entrenamiento\n",
        "NUM_EPOCHS = 8  # Número de épocas de entrenamiento\n",
        "lr = 1e-5 # Tasa de aprendizaje\n",
        "batch_s = 32 # Tamaño del lote (batch size)\n",
        "final_model_name = '/content/gdrive/MyDrive/temp/unlearning_fine-tuning-basic_20-60.h5' # Ruta donde se guardará el modelo\n",
        "\n",
        "\n",
        "# Cargar el modelo preentrenado\n",
        "model =  tf.keras.models.load_model('./NoriegaVazquezPablo_TFG_CODE/models/originalModel.h5')\n",
        "\n",
        "# Definir parámetros de aumento de datos\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "# Convertir las etiquetas one-hot a etiquetas de clase\n",
        "# y_train_classes = np.argmax(Y_train_onehot, axis=1)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mean_absolute_error')\n",
        "\n",
        "# Definir el generador para los datos de entrenamiento con aumento de datos\n",
        "train_generator = datagen.flow(X_train, Y_train, batch_size=batch_s)\n",
        "valid_generator = datagen.flow(X_valid, Y_valid, batch_size=batch_s)\n",
        "\n",
        "# Definir los callbacks\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
        "mc = tf.keras.callbacks.ModelCheckpoint(final_model_name, monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "# Entrenar el modelo\n",
        "history = model.fit(train_generator, validation_data=valid_generator, batch_size=batch_s, epochs=NUM_EPOCHS, shuffle=True, verbose=1, callbacks=[es, mc])\n",
        "\n",
        "# Guardar el historial de entrenamiento\n",
        "with open(final_model_name.replace('.h5', ''), 'wb') as handle:\n",
        "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Guardar el modelo final\n",
        "model.save(final_model_name.replace('.h5', '_final.h5'))\n"
      ],
      "metadata": {
        "id": "9r7kf2pZRw0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ENTRENAR MODELO UNLEARNING 2 (FINE-TUNING CON ETIQUETAS RANDOM)**"
      ],
      "metadata": {
        "id": "odiJFowsR1QF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Cargar los datos**: Se cargan los datos de entrenamiento, validación y prueba desde archivos `.npy` ubicados en el directorio `./NoriegaVazquezPablo_TFG_CODE/originalData`. En este conjunto se han borrado las muestras entre 20 y 28 años.\n",
        "\n",
        "2. **Normalizar etiquetas**: Se normalizan las etiquetas dividiéndolas por 60. Esto se aplica a las etiquetas de entrenamiento (`Y_train`), validación (`Y_valid`) y prueba (`Y_test`).\n",
        "\n",
        "3. **Verificar dimensiones**: Se imprimen las dimensiones de los datos cargados para asegurar que todo se ha cargado correctamente y las dimensiones son las esperadas."
      ],
      "metadata": {
        "id": "k_sb2yuDmR_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los conjuntos de datos\n",
        "X_train = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'X_train.npy'))\n",
        "Y_train = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'Y_train.npy'))\n",
        "Y_train = Y_train / 60  # Normalizar etiquetas de entrenamiento\n",
        "\n",
        "X_valid = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'X_valid.npy'))\n",
        "Y_valid = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'Y_valid.npy'))\n",
        "Y_valid = Y_valid / 60  # Normalizar etiquetas de validación\n",
        "\n",
        "X_test = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'X_test.npy'))\n",
        "Y_test = np.load(os.path.join('./NoriegaVazquezPablo_TFG_CODE/originalData', 'Y_test.npy'))\n",
        "Y_test = Y_test / 60  # Normalizar etiquetas de prueba\n",
        "\n",
        "# Definir los límites de edad\n",
        "edad_min = 20\n",
        "edad_max = 28\n",
        "\n",
        "# Encontrar índices donde la edad está dentro del rango deseado\n",
        "indices_train = np.where((Y_train>= edad_min) & (Y_train <= edad_max))[0]\n",
        "indices_valid = np.where((Y_valid >= edad_min) & (Y_valid<= edad_max))[0]\n",
        "indices_test = np.where((Y_test >= edad_min) & (Y_test <= edad_max))[0]\n",
        "\n",
        "Y_train_regression = Y_train / 60\n",
        "Y_valid_regression = Y_valid/60\n",
        "Y_test_regression = Y_test/60\n",
        "\n",
        "# Imprimir el número de muestras encontradas en cada conjunto\n",
        "print(\"Número de muestras en el rango de edad (20-28 años):\")\n",
        "print(\"Conjunto de entrenamiento:\", len(indices_train))\n",
        "\n",
        "print(\"Dimensiones de los conjuntos de entrenamiento:\")\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"Y_train_onehot:\", Y_train_regression.shape)\n",
        "\n",
        "print(\"\\nDimensiones de los conjuntos de validación:\")\n",
        "print(\"X_valid:\", X_valid.shape)\n",
        "print(\"Y_valid_onehot:\", Y_valid_regression.shape)\n",
        "\n",
        "print(\"\\nDimensiones de los conjuntos de test:\")\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"Y_test_onehot:\", Y_test_regression.shape)"
      ],
      "metadata": {
        "id": "khSUWupmR6Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocesar conjunto de entrenamiento, validación y prueba**:\n",
        "   - Se convierten los datos a tipo `float32` para asegurar que están en el formato correcto para la red neuronal.\n",
        "   - Para cada imagen, se expande una dimensión adicional para que tenga la forma esperada por la función `preprocess_input` de ResNet50.\n",
        "   - Luego, se aplica `preprocess_input` de ResNet50 para normalizar las imágenes de acuerdo con los requisitos del modelo preentrenado.\n",
        "\n",
        "Esta preprocesamiento es necesario porque ResNet50, como muchas redes neuronales preentrenadas, espera que las imágenes de entrada estén normalizadas de una manera específica, lo cual ayuda a mejorar la precisión y la convergencia del modelo.\n"
      ],
      "metadata": {
        "id": "qUVw77xImXAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "X_train = X_train.astype('float32')\n",
        "for i in range(0,X_train.shape[0]):\n",
        "  x = X_train[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_train[i,] = tf.keras.applications.resnet50.preprocess_input(x)\n",
        "\n",
        "# validation set\n",
        "X_valid = X_valid.astype('float32')\n",
        "for i in range(0,X_valid.shape[0]):\n",
        "  x = X_valid[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_valid[i,] = tf.keras.applications.resnet50.preprocess_input(x)\n",
        "\n",
        "X_test = X_test.astype('float32')\n",
        "for i in range(0,X_test.shape[0]):\n",
        "  x = X_test[i,:,:,:]\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  X_test[i,] = tf.keras.applications.resnet50.preprocess_input(x)"
      ],
      "metadata": {
        "id": "kMInb3-hTBvg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###En esta celda de código, se llevan a cabo las siguientes acciones:\n",
        "\n",
        "1. **Cargar el modelo preentrenado**:\n",
        "   - Se carga un modelo preentrenado desde la ubicación especificada en el disco.\n",
        "\n",
        "2. **Definir los parámetros de entrenamiento**:\n",
        "   - Se establecen los parámetros necesarios para el entrenamiento del modelo, como el número de epochs (`NUM_EPOCHS`), la tasa de aprendizaje (`lr`), y el tamaño del lote (`batch_s`).\n",
        "\n",
        "3. **Definir el generador de datos de aumento** (`ImageDataGenerator`):\n",
        "   - Se utiliza `ImageDataGenerator()` para definir los parámetros de aumento de datos. Esta función se utiliza para aumentar la cantidad de datos de entrenamiento mediante la aplicación de transformaciones aleatorias a las imágenes.\n",
        "\n",
        "4. **Compilar el modelo**:\n",
        "   - Se compila el modelo utilizando el optimizador Adam con la tasa de aprendizaje especificada (`lr`) y la función de pérdida de error absoluto medio (`mean_absolute_error`).\n",
        "\n",
        "5. **Definir la función `custom_generator`**:\n",
        "   - Se define una función generadora personalizada (`custom_generator`) que se utilizará para generar lotes de datos de entrenamiento con modificaciones selectivas en las edades. Esta función toma como entrada los datos de entrada (`X`) y las edades de (`Y`), el tamaño del lote (`batch_size`) y una lista de índices (`index_indices`) que indica en qué índices se deben realizar las modificaciones de las edades.\n",
        "   - La función generadora baraja aleatoriamente los índices en cada epoch y luego crea lotes de datos de tamaño `batch_size`. Para los índices especificados en `index_indices`, la función modifica las edades sumándoles un número aleatorio fuera del rango [-2, -1, 0, 1, 2], asegurándose de que la nueva edad esté dentro del rango [20, 60].\n",
        "   - Esto permite simular el escenario en el que se desea introducir un cierto nivel de ruido en los datos de entrenamiento para mejorar la robustez del modelo.\n",
        "\n",
        "6. **Definir los generadores de datos**:\n",
        "   - Se define un generador de datos personalizado para el entrenamiento utilizando la función `custom_generator` y otro generador de datos estándar para la validación. Estos generadores se utilizarán para proporcionar lotes de datos al modelo durante el entrenamiento y la validación.\n",
        "\n",
        "7. **Definir callbacks**:\n",
        "   - Se establecen callbacks para el entrenamiento del modelo. En este caso, se utiliza `EarlyStopping` para detener el entrenamiento prematuramente si la pérdida en el conjunto de validación deja de disminuir, y `ModelCheckpoint` para guardar el mejor modelo obtenido durante el entrenamiento.\n",
        "\n",
        "8. **Entrenar el modelo**:\n",
        "   - Se entrena el modelo utilizando el generador de datos personalizado para el entrenamiento y el generador de datos estándar para la validación. Se especifica el número de épocas (`NUM_EPOCHS`), el número de pasos por época (`steps_per_epoch`), y se utilizan los callbacks definidos anteriormente.\n",
        "\n",
        "9. **Guardar el historial de entrenamiento**:\n",
        "   - Se guarda el historial de entrenamiento del modelo en un archivo utilizando la biblioteca `pickle`. Esto permite realizar un seguimiento del progreso del entrenamiento y analizar las métricas de rendimiento a lo largo del tiempo.\n",
        "\n",
        "10. **Guardar el modelo final**:\n",
        "    - Se guarda el modelo final entrenado en un archivo con el nombre especificado. Este modelo puede ser utilizado posteriormente para hacer predicciones sobre nuevos datos.\n"
      ],
      "metadata": {
        "id": "U6E_oEZen3zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir los parámetros de entrenamiento\n",
        "NUM_EPOCHS = 8  # Número de épocas de entrenamiento\n",
        "lr = 1e-5  # Tasa de aprendizaje\n",
        "batch_s = 32  # Tamaño del lote (batch size)\n",
        "final_model_name = '/content/gdrive/MyDrive/temp/unlearning_sum_36-44_16.h5' #nombre final del modelo\n",
        "\n",
        "# Cargar el modelo preentrenado\n",
        "model =  tf.keras.models.load_model('./NoriegaVazquezPablo_TFG_CODE/models/originalModel.h5')\n",
        "\n",
        "\n",
        "# Definir el generador de datos de aumento (ImageDataGenerator)\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mean_absolute_error')\n",
        "\n",
        "def custom_generator(X, Y, batch_size, index_indices):\n",
        "    num_samples = len(X)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    while True:\n",
        "        np.random.shuffle(indices)  # Barajar aleatoriamente los índices en cada epoch\n",
        "\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[start:start+batch_size]\n",
        "            x_batch = X[batch_indices]\n",
        "            y_batch = Y[batch_indices].copy()  # Copiar las edades para modificarlas\n",
        "\n",
        "            # Modificar las edades selectivamente en los índices deseados antes de asignar al lote\n",
        "            for i, idx in enumerate(batch_indices):\n",
        "                if idx in index_indices:\n",
        "                    # Generar un número aleatorio fuera de [-2, -1, 0, 1, 2]\n",
        "                    random_offset = np.random.randint(-6, 7)\n",
        "                    while random_offset in [-2, -1, 0, 1, 2]:  # Verificar si está en la lista prohibida\n",
        "                        random_offset = np.random.randint(-6, 7)  # Volver a generar si es necesario\n",
        "                    # Sumar este número aleatorio al valor original de la edad\n",
        "                    new_age = y_batch[i] + random_offset\n",
        "                    # Asegurar que la nueva edad esté dentro del rango [20, 60]\n",
        "                    y_batch[i] = np.clip(new_age, 20, 60)\n",
        "\n",
        "            yield x_batch, y_batch\n",
        "\n",
        "# Definir el generador de datos para el entrenamiento con modificación selectiva de edades\n",
        "train_generator_custom = custom_generator(X_train, Y_train_regression, batch_s, indices_train)\n",
        "\n",
        "# Definir el generador de datos de validación estándar\n",
        "valid_generator_standard = datagen.flow(X_valid, Y_valid_regression, batch_size=batch_s)\n",
        "\n",
        "# Definir callbacks (detener temprano y guardar el mejor modelo)\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=15)\n",
        "mc = tf.keras.callbacks.ModelCheckpoint(final_model_name, monitor='val_loss', mode='min', save_best_only=True)\n",
        "\n",
        "# Entrenar el modelo utilizando el generador de datos personalizado y el generador de datos de validación estándar\n",
        "history = model.fit(train_generator_custom, validation_data=valid_generator_standard, epochs=NUM_EPOCHS,\n",
        "                    steps_per_epoch=len(X_train) // batch_s, shuffle=True, verbose=1, callbacks=[es, mc])\n",
        "\n",
        "# Guardar el historial de entrenamiento\n",
        "with open(final_model_name.replace('.h5', ''), 'wb') as handle:\n",
        "    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "yPYBXcZgTQ4A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}